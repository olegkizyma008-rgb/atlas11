#!/usr/bin/env python3
"""
RAG Indexer –¥–ª—è KONTUR v12 "–ö–æ–∑–∏—Ä" ‚Äî MLX Edition
–Ü–Ω–¥–µ–∫—Å—É—î AppleScript, JXA, —Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é macOS –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
Semantic chunking + –∫–æ–Ω—Ç–µ–∫—Å—Ç + GPU acceleration
"""
import os
import sys
from pathlib import Path
from typing import Generator, Tuple, List

# –û–ø—Ü—ñ–π–Ω–æ: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ MLX –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞ Apple Silicon
USE_MLX = os.getenv("USE_MLX", "1") in ("1", "true", "yes")
MLX_READY = False
try:
    if USE_MLX:
        import numpy as np
        from mlx_lm import load as mlx_load
        MLX_READY = True
except Exception:
    MLX_READY = False

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
# –í–∏–∑–Ω–∞—á–∞—î–º–æ —à–ª—è—Ö–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –ø—Ä–æ–µ–∫—Ç—É
PROJECT_ROOT = Path(__file__).parent.parent
KNOWLEDGE_SOURCES_DIRS = [
    PROJECT_ROOT / "rag" / "knowledge_sources",
    PROJECT_ROOT / "rag" / "knowledge_base" / "large_corpus",
    PROJECT_ROOT / "rag" / "macOS-automation-knowledge-base",
]
CHROMA_PERSIST_DIR = PROJECT_ROOT / "rag" / "chroma_mac"
EMBEDDING_MODEL = "BAAI/bge-m3"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
SEMANTIC_THRESHOLD = 95  # Percentile –¥–ª—è semantic chunking

# === –í–ê–õ–Ü–î–ù–Ü –†–û–ó–®–ò–†–ï–ù–ù–Ø ===
VALID_EXTENSIONS = {
    '.applescript': 'AppleScript',
    '.scpt': 'AppleScript (compiled)',
    '.js': 'JXA',
    '.jxa': 'JXA',
    '.md': 'Documentation',
    '.txt': 'Text',
    '.sh': 'Shell Script'
}

def find_files() -> Generator[Tuple[Path, str, Path], None, None]:
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ñ–∞–π–ª–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –∑ —É—Å—ñ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π"""
    for source_dir in KNOWLEDGE_SOURCES_DIRS:
        source_path = Path(source_dir)
        if not source_path.exists():
            continue
        for ext, doc_type in VALID_EXTENSIONS.items():
            for path in source_path.rglob(f"*{ext}"):
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ node_modules, .git —Ç–∞ –≤–µ–ª–∏–∫—ñ —Ñ–∞–π–ª–∏
                if 'node_modules' in str(path) or '.git' in str(path):
                    continue
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ñ–∞–π–ª–∏ –±—ñ–ª—å—à—ñ 1MB (–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è 80GB —Å–∏—Ç—É–∞—Ü—ñ—è–º)
                try:
                    if path.stat().st_size > 1_000_000:
                        continue
                except:
                    continue
                yield path, doc_type, source_path

def read_file_safe(path: Path) -> str:
    """–ë–µ–∑–ø–µ—á–Ω–µ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∫–æ–¥—É–≤–∞–Ω–Ω—è–º–∏"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    for encoding in encodings:
        try:
            return path.read_text(encoding=encoding)
        except (UnicodeDecodeError, UnicodeError):
            continue
    return ""

def extract_file_context(content: str, max_length: int = 300) -> str:
    """–í–∏—Ç—è–≥—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ–∞–π–ª—É (–ø–µ—Ä—à—ñ —Ä—è–¥–∫–∏)"""
    lines = content.split('\n')[:5]
    context = '\n'.join(lines)
    return context[:max_length]

def create_semantic_chunks(content: str, embeddings_fn) -> List[str]:
    """–†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ semantic chunks –Ω–∞ –æ—Å–Ω–æ–≤—ñ embedding similarity"""
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    # –°–ø–æ—á–∞—Ç–∫—É —Ä–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ –±–∞–∑–æ–≤—ñ chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    
    base_chunks = text_splitter.split_text(content)
    
    if len(base_chunks) <= 1:
        return base_chunks
    
    # –î–ª—è semantic chunking –ø–æ—Ç—Ä–µ–±—É—î–º–æ embeddings
    try:
        # –ì–µ–Ω–µ—Ä—É—î–º–æ embeddings –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ chunk
        chunk_embeddings = embeddings_fn(base_chunks)
        
        # –û–±—á–∏—Å–ª—é—î–º–æ similarity –º—ñ–∂ —Å—É—Å—ñ–¥–Ω—ñ–º–∏ chunks
        import numpy as np
        semantic_chunks = []
        current_chunk = base_chunks[0]
        
        for i in range(1, len(base_chunks)):
            # –û–±—á–∏—Å–ª—é—î–º–æ cosine similarity
            emb1 = np.array(chunk_embeddings[i-1])
            emb2 = np.array(chunk_embeddings[i])
            
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            
            # –Ø–∫—â–æ similarity –Ω–∏–∑—å–∫–∞ ‚Äî —Ü–µ –º–µ–∂–∞ chunk'–∞
            if similarity < 0.5:  # Threshold –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
                semantic_chunks.append(current_chunk)
                current_chunk = base_chunks[i]
            else:
                # –û–±'—î–¥–Ω—É—î–º–æ chunks —è–∫—â–æ –≤–æ–Ω–∏ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ –±–ª–∏–∑—å–∫—ñ
                current_chunk += "\n\n" + base_chunks[i]
        
        semantic_chunks.append(current_chunk)
        return semantic_chunks
    except Exception:
        # Fallback –Ω–∞ –±–∞–∑–æ–≤—ñ chunks —è–∫—â–æ semantic chunking –Ω–µ –ø—Ä–∞—Ü—é—î
        return base_chunks

def main():
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    
    console = Console()
    console.print("[bold green]üöÄ RAG Indexer v12 ‚Äî KONTUR '–ö–æ–∑–∏—Ä'[/bold green]")
    
    # === –ö–†–û–ö 1: –ó–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ ===
    console.print("\n[cyan]üìÇ –ü–æ—à—É–∫ —Ñ–∞–π–ª—ñ–≤...[/cyan]")
    files_to_index = list(find_files())
    console.print(f"[green]‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(files_to_index)} —Ñ–∞–π–ª—ñ–≤[/green]")
    
    if not files_to_index:
        console.print("[red]‚ùå –§–∞–π–ª–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ![/red]")
        return
    
    # === –ö–†–û–ö 2: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Embeddings (–ø–æ—Ç—Ä–µ–±—É—î–º–æ –¥–ª—è semantic chunking) ===
    console.print("\n[cyan]üß† –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è embedding –º–æ–¥–µ–ª—ñ...[/cyan]")
    
    if MLX_READY:
        console.print("[green]‚ö° –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è MLX (bge-m3) –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞ Apple Silicon[/green]")
        model, tokenizer = mlx_load(EMBEDDING_MODEL)

        def embed_texts(texts: List[str]):
            outputs = []
            for t in texts:
                tokens = tokenizer(t, return_tensors="np", padding=True, truncation=True)
                hidden = model(**tokens).last_hidden_state
                vec = hidden.mean(axis=1)[0]
                outputs.append(vec.tolist())
            return outputs

        embeddings_fn = embed_texts
    else:
        console.print("[yellow]MLX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é HuggingFaceEmbeddings.[/yellow]")
        from langchain_huggingface import HuggingFaceEmbeddings
        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        embeddings_fn = embedding_model.embed_documents
    
    # === –ö–†–û–ö 3: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ semantic chunking + –∫–æ–Ω—Ç–µ–∫—Å—Ç ===
    console.print("\n[cyan]üìù –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (semantic chunking + –∫–æ–Ω—Ç–µ–∫—Å—Ç)...[/cyan]")
    
    from langchain_core.documents import Document
    
    documents = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤...", total=len(files_to_index))
        
        for path, doc_type, base_path in files_to_index:
            content = read_file_safe(path)
            if not content or len(content.strip()) < 50:
                progress.advance(task)
                continue
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–æ–∫—É–º–µ–Ω—Ç –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
            try:
                source_name = str(path.relative_to(base_path))
            except ValueError:
                source_name = str(path.name)
            
            # Semantic chunking
            chunks = create_semantic_chunks(content, embeddings_fn)
            file_context = extract_file_context(content)
            
            for i, chunk in enumerate(chunks):
                # –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ç–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π chunk
                prev_chunk = chunks[i-1][-150:] if i > 0 else ""
                next_chunk = chunks[i+1][:150] if i < len(chunks)-1 else ""
                
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": source_name,
                        "type": doc_type,
                        "chunk": i,
                        "total_chunks": len(chunks),
                        "file_context": file_context,
                        "prev_chunk_context": prev_chunk,
                        "next_chunk_context": next_chunk,
                    }
                )
                documents.append(doc)
            
            progress.advance(task)
    
    console.print(f"[green]‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫—ñ–≤ (semantic + –∫–æ–Ω—Ç–µ–∫—Å—Ç)[/green]")
    
    # === –ö–†–û–ö 4: –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ Chroma –∑ MLX ===
    console.print("\n[cyan]üíæ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ ChromaDB (–∑ MLX GPU acceleration)...[/cyan]")
    
    from langchain_chroma import Chroma
    
    # Batch indexing –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    BATCH_SIZE = 100
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è...", total=len(documents))
        
        db = Chroma(
            persist_directory=str(CHROMA_PERSIST_DIR),
            embedding_function=embeddings_fn if MLX_READY else None
        )
        
        for i in range(0, len(documents), BATCH_SIZE):
            batch = documents[i:i + BATCH_SIZE]
            db.add_documents(batch)
            progress.advance(task, advance=len(batch))
    
    # === –§–Ü–ù–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢ ===
    console.print("\n" + "="*50)
    console.print("[bold green]‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê![/bold green]")
    console.print(f"[cyan]üìä –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–æ–¥–∞–Ω–æ: {len(documents)}[/cyan]")
    console.print(f"[cyan]üìÅ –ë–∞–∑–∞: {CHROMA_PERSIST_DIR}[/cyan]")
    console.print(f"[cyan]üß† Embedding –º–æ–¥–µ–ª—å: {EMBEDDING_MODEL}[/cyan]")
    console.print(f"[cyan]‚ö° GPU acceleration: {'MLX (M1 Max)' if MLX_READY else 'CPU'}[/cyan]")
    console.print(f"[cyan]üîÄ Semantic chunking: ‚úÖ –£–í–Ü–ú–ö–ù–ï–ù–û[/cyan]")
    console.print(f"[cyan]üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç: ‚úÖ –î–û–î–ê–ù–û (prev/next/file)[/cyan]")
    console.print("="*50)

if __name__ == "__main__":
    main()
