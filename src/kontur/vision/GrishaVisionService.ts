/**
 * GrishaVisionService - Unified Vision Service for Grisha
 * 
 * Supports both modes:
 * - LIVE: Continuous streaming to Gemini Live (real-time observation)
 * - ON-DEMAND: Screenshot after each step, analyzed by Copilot/GPT-4o
 * 
 * Both modes share the same interface for window selection and verification
 */

import { EventEmitter } from 'events';
import { desktopCapturer } from 'electron';
import { getVisionConfig } from '../providers/config';
import { getProviderRouter } from '../providers/router';
import { VisionResponse } from '../providers/types';

export interface VisionObservationResult {
    type: 'confirmation' | 'alert' | 'observation' | 'verification';
    message: string;
    verified?: boolean; // For on-demand: did the task succeed?
    confidence?: number;
    anomalies?: Array<{
        type: string;
        severity: 'low' | 'medium' | 'high';
        description: string;
    }>;
    timestamp: number;
    mode: 'live' | 'on-demand';
}

export interface ScreenSource {
    id: string;
    name: string;
    thumbnail: string;
    isScreen: boolean;
}

export class GrishaVisionService extends EventEmitter {
    private isObserving: boolean = false;
    private isPaused: boolean = false;
    private captureInterval: NodeJS.Timeout | null = null;
    private captureIntervalMs: number = 2000; // 2s = 0.5 FPS (optimized for API)
    private geminiLive: any = null;
    private frameCount: number = 0;
    private isSpeaking: boolean = false;

    // Selected source for capture
    private selectedSourceId: string | null = null;
    private selectedSourceName: string | null = null;

    constructor() {
        super();
    }

    /**
     * Get current Vision mode from config
     */
    get mode(): 'live' | 'on-demand' {
        return getVisionConfig().mode;
    }

    /**
     * Set Gemini Live service (for live mode)
     */
    setGeminiLive(geminiLive: any) {
        this.geminiLive = geminiLive;

        if (geminiLive) {
            geminiLive.on('text', (text: string) => {
                this.processLiveResponse(text);
            });

            geminiLive.on('audio', (audio: any) => {
                if (!this.isSpeaking) {
                    this.isSpeaking = true;
                    console.log('[GRISHA VISION] üîá Audio started - pausing frame capture');
                    setTimeout(() => {
                        if (this.isSpeaking) {
                            console.log('[GRISHA VISION] ‚è±Ô∏è Audio timeout - resuming');
                            this.isSpeaking = false;
                        }
                    }, 5000);
                }
                this.emit('audio', audio);
            });

            geminiLive.on('turnComplete', () => {
                console.log('[GRISHA VISION] üé§ Turn complete');
                this.isSpeaking = false;
                this.emitResult('confirmation', 'Grisha finished speaking', true);
            });
        }
    }

    /**
     * Get available screen/window sources
     */
    async getSources(): Promise<ScreenSource[]> {
        try {
            const sources = await desktopCapturer.getSources({
                types: ['window', 'screen'],
                thumbnailSize: { width: 150, height: 100 }
            });

            return sources.map(source => ({
                id: source.id,
                name: source.name,
                thumbnail: source.thumbnail.toDataURL(),
                isScreen: source.id.startsWith('screen:')
            }));
        } catch (err) {
            console.error('[GRISHA VISION] Failed to get sources:', err);
            return [];
        }
    }

    /**
     * Select a specific source (window/screen) for capture
     */
    selectSource(sourceId: string, sourceName: string) {
        this.selectedSourceId = sourceId;
        this.selectedSourceName = sourceName;
        console.log(`[GRISHA VISION] üéØ Selected source: ${sourceName} (${sourceId})`);
        this.emit('source_changed', { id: sourceId, name: sourceName });
    }

    /**
     * Auto-select source by app name
     */
    async autoSelectSource(appName: string): Promise<boolean> {
        const sources = await this.getSources();

        // Filter out Atlas/Electron windows to avoid self-capture
        const externalSources = sources.filter(s =>
            !s.name.toLowerCase().includes('electron') &&
            !s.name.toLowerCase().includes('atlas') &&
            !s.name.toLowerCase().includes('kontur')
        );

        console.log(`[GRISHA VISION] üîç Looking for "${appName}" among ${externalSources.length} external windows`);

        const normalize = (s: string) => s.toLowerCase().trim();
        const target = normalize(appName);

        // Common mappings (UA <-> EN)
        const ALIASES: Record<string, string[]> = {
            '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä': ['calculator'],
            'calculator': ['–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä'],
            '—Ç–µ—Ä–º—ñ–Ω–∞–ª': ['terminal', 'iterm'],
            'terminal': ['—Ç–µ—Ä–º—ñ–Ω–∞–ª', 'iterm'],
            '–Ω–æ—Ç–∞—Ç–∫–∏': ['notes'],
            'notes': ['–Ω–æ—Ç–∞—Ç–∫–∏'],
            '—Å–∞—Ñ–∞—Ä—ñ': ['safari'],
            'safari': ['—Å–∞—Ñ–∞—Ä—ñ'],
            '—Ñ–∞–π–Ω–¥–µ—Ä': ['finder'],
            'finder': ['—Ñ–∞–π–Ω–¥–µ—Ä']
        };

        const searchTerms = [target, ...(ALIASES[target] || [])];

        let matched = externalSources.find(s =>
            searchTerms.some(term => normalize(s.name) === term)
        );

        if (!matched) {
            matched = externalSources.find(s =>
                searchTerms.some(term => normalize(s.name).includes(term))
            );
        }

        if (matched) {
            console.log(`[GRISHA VISION] ‚úÖ Found window: "${matched.name}" (matched for "${appName}")`);
            this.selectSource(matched.id, matched.name);
            return true;
        }

        console.warn(`[GRISHA VISION] ‚ö†Ô∏è Window not found for: "${appName}". Available: ${externalSources.map(s => s.name).join(', ')}`);
        return false;
    }

    /**
     * Clear source selection (capture entire screen)
     */
    clearSourceSelection() {
        this.selectedSourceId = null;
        this.selectedSourceName = null;
        console.log('[GRISHA VISION] üñ•Ô∏è Using full screen capture');
    }

    /**
     * Start observation (works for both modes)
     */
    async startObservation(taskDescription?: string) {
        if (this.isObserving) return;

        const currentMode = this.mode;
        console.log(`[GRISHA VISION] üëÅÔ∏è Starting observation [${currentMode.toUpperCase()}]...`);
        this.isObserving = true;
        this.frameCount = 0;

        if (currentMode === 'live') {
            await this.startLiveObservation(taskDescription);
        } else {
            // On-demand: just mark as observing, capture happens per-step
            this.emitResult('observation', `On-Demand —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ. ${taskDescription || '–ì–æ—Ç–æ–≤–∏–π –¥–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏.'}`);
        }
    }

    /**
     * Stop observation
     */
    stopObservation() {
        if (!this.isObserving) return;

        console.log(`[GRISHA VISION] üëÅÔ∏è Observation stopped after ${this.frameCount} frames`);
        this.isObserving = false;
        this.isSpeaking = false;
        this.isPaused = false;

        if (this.captureInterval) {
            clearInterval(this.captureInterval);
            this.captureInterval = null;
        }

        this.emitResult('confirmation', `–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ ${this.frameCount} –∫–∞–¥—Ä—ñ–≤.`);
    }

    /**
     * Pause capture (during step execution)
     */
    pauseCapture() {
        if (!this.isPaused) {
            this.isPaused = true;
            console.log('[GRISHA VISION] ‚è∏Ô∏è Capture paused');
        }
    }

    /**
     * Resume capture (for step verification)
     */
    resumeCapture() {
        if (this.isPaused) {
            this.isPaused = false;
            console.log('[GRISHA VISION] ‚ñ∂Ô∏è Capture resumed');
            // Send immediate frame on resume for verification
            if (this.mode === 'live') {
                this.captureAndSendLiveFrame();
            }
        }
    }

    /**
     * Verify a step was executed (On-Demand mode)
     * Captures screenshot and sends to Copilot/GPT-4o for analysis
     */
    /**
     * Verify a step was executed
     */
    async verifyStep(stepAction: string, stepDetails?: string): Promise<VisionObservationResult> {
        console.log(`[GRISHA VISION] üîç Verifying step: ${stepAction}`);

        // If ON-DEMAND mode, run directly
        if (this.mode === 'on-demand') {
            return this.verifyStepOnDemand(stepAction, stepDetails);
        }

        // Live mode with fallback
        return new Promise(async (resolve) => {
            // Notify Gemini Live
            await this.notifyActionLive(stepAction, stepDetails || '');

            const cleanup = () => {
                this.removeListener('observation', responseHandler);
            };

            const responseHandler = (result: VisionObservationResult) => {
                if (result.type === 'confirmation' || result.type === 'alert') {
                    cleanup();
                    resolve(result);
                }
            };

            this.on('observation', responseHandler);

            // Timeout with FALLBACK
            setTimeout(async () => {
                cleanup();
                console.warn('[GRISHA VISION] ‚ö†Ô∏è Verification timeout (Live Mode). Falling back to On-Demand verification...');

                try {
                    // FALLBACK: Try GPT-4o / Copilot analysis as backup
                    const fallbackResult = await this.verifyStepOnDemand(stepAction, stepDetails);
                    resolve(fallbackResult);
                } catch (e) {
                    resolve({
                        type: 'alert',
                        message: 'Timeout & Fallback Failed: Gemini Live did not respond and On-Demand analysis failed.',
                        verified: false,
                        timestamp: Date.now(),
                        mode: 'live'
                    });
                }
            }, 10000); // 10s timeout before fallback
        });
    }

    /**
     * Check if an object/window is visible on screen
     * Returns visibility check result
     */
    private async checkObjectVisibility(stepAction: string, base64Image: string): Promise<{ visible: boolean, message: string }> {
        try {
            const router = getProviderRouter();

            // Extract object/app name from step action
            const objectMatch = stepAction.match(/(?:–≤—ñ–¥–∫—Ä–∏—Ç–∏|open|launch|–≤ –ø—Ä–æ–≥—Ä–∞–º—ñ|in|—É|click|–Ω–∞—Ç–∏—Å–Ω–∏|type in)\s+([A-Za-z–ê-–Ø–∞-—è—ñ–Ü—ó–á—î–Ñ0-9\s]+)/i);
            const objectName = objectMatch ? objectMatch[1].trim() : '–æ–±\'—î–∫—Ç';

            const visibilityPrompt = `
–ê–ù–ê–õ–Ü–ó –í–ò–î–ò–ú–û–°–¢–Ü:
–ó–∞–≤–¥–∞–Ω–Ω—è: "${stepAction}"
–û–±'—î–∫—Ç/–≤—ñ–∫–Ω–æ –¥–ª—è –ø–æ—à—É–∫—É: "${objectName}"

–í–ê–ñ–õ–ò–í–û:
- –Ü–≥–Ω–æ—Ä—É–π —Ç–µ–∫—Å—Ç–æ–≤—ñ –ª–æ–≥–∏, –∫–æ–Ω—Å–æ–ª—å –∞–±–æ —á–∞—Ç, –¥–µ –Ω–∞–ø–∏—Å–∞–Ω–æ –ø—Ä–æ —Ü–µ–π –æ–±'—î–∫—Ç.
- –¢–∏ –ø–æ–≤–∏–Ω–µ–Ω –±–∞—á–∏—Ç–∏ –°–ê–ú –Ü–ù–¢–ï–†–§–ï–ô–° –ø—Ä–æ–≥—Ä–∞–º–∏ (–∫–Ω–æ–ø–∫–∏, –ø–æ–ª—è, –≤—ñ–∫–Ω–æ).
- –Ø–∫—â–æ —Ç–∏ –±–∞—á–∏—à —Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç "Calculator opened" –∞–±–æ –ø–æ–¥—ñ–±–Ω–µ –≤ –ª–æ–≥–∞—Ö - —Ü–µ invisible.
- –Ø–∫—â–æ –≤—ñ–∫–Ω–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç–æ —ñ–Ω—à–∏–º (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ ATLAS KONTUR) - —Ü–µ invisible.

–í–Ü–î–ü–û–í–Ü–î–¨ –ù–ê –ü–ò–¢–ê–ù–ù–Ø:
1. –ß–∏ –±–∞—á–∏—à —Ç–∏ –Ü–ù–¢–ï–†–§–ï–ô–° –ø—Ä–æ–≥—Ä–∞–º–∏ "${objectName}"?
2. –Ø–∫—â–æ —Ç–∞–∫ - –æ–ø–∏—à–∏ —è–∫ –≤—ñ–Ω –≤–∏–≥–ª—è–¥–∞—î (–∫–æ–ª—ñ—Ä, –µ–ª–µ–º–µ–Ω—Ç–∏)?
3. –Ø–∫—â–æ –Ω—ñ - —â–æ —Å–∞–º–µ –ø–µ—Ä–µ–∫—Ä–∏–≤–∞—î –π–æ–≥–æ?

–§–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ JSON:
{
  "visible": true/false,
  "location": "–æ–ø–∏—Å –¥–µ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è" –∞–±–æ null,
  "screen_content": "—â–æ –≤–∏–¥–Ω–æ –Ω–∞ –µ–∫—Ä–∞–Ω—ñ",
  "is_obscured_by_atlas": true/false
}`;

            const response = await router.analyzeVision({
                image: base64Image,
                mimeType: 'image/jpeg',
                taskContext: stepAction,
                prompt: visibilityPrompt
            });

            // Parse visibility response
            try {
                const analysis = response.analysis;
                const jsonMatch = analysis.match(/\{[\s\S]*\}/);
                if (jsonMatch) {
                    const parsed = JSON.parse(jsonMatch[0]);
                    const visible = parsed.visible === true;

                    if (visible) {
                        const location = parsed.location || '–Ω–∞ –µ–∫—Ä–∞–Ω—ñ';
                        return {
                            visible: true,
                            message: `–ë–∞—á—É "${objectName}" ${location}`
                        };
                    } else {
                        const screenContent = parsed.screen_content || '—ñ–Ω—à–∏–π –≤–º—ñ—Å—Ç';
                        return {
                            visible: false,
                            message: `–ù–µ –±–∞—á—É "${objectName}" –Ω–∞ –µ–∫—Ä–∞–Ω—ñ. –í–∏–¥–Ω–æ: ${screenContent}`
                        };
                    }
                }
            } catch (parseErr) {
                console.warn('[GRISHA VISION] Could not parse visibility JSON, analyzing text:', response.analysis);
            }

            // Fallback: analyze text response
            const analysisLower = response.analysis.toLowerCase();
            const positiveIndicators = ['–±–∞—á—É', 'yes', 'visible', '–≤—ñ–¥–∫—Ä–∏—Ç–æ', 'opened', 'present'];
            const negativeIndicators = ['–Ω–µ –±–∞—á—É', 'no', 'not visible', '–∑–∞–∫—Ä–∏—Ç–æ', 'hidden', 'absent', 'missing'];

            const hasPositive = positiveIndicators.some(ind => analysisLower.includes(ind));
            const hasNegative = negativeIndicators.some(ind => analysisLower.includes(ind));

            if (hasNegative || !hasPositive) {
                return {
                    visible: false,
                    message: `–ù–µ –±–∞—á—É "${objectName}" –Ω–∞ –µ–∫—Ä–∞–Ω—ñ. ${response.analysis.slice(0, 100)}`
                };
            }

            return {
                visible: true,
                message: `–ë–∞—á—É "${objectName}". ${response.analysis.slice(0, 100)}`
            };

        } catch (error: any) {
            console.error('[GRISHA VISION] Visibility check failed:', error);
            return {
                visible: false,
                message: `–ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –≤–∏–¥–∏–º–æ—Å—Ç—ñ: ${error.message}`
            };
        }
    }

    /**
     * Private: On-Demand Verification Logic
     * NOW WITH VISIBILITY CHECK FIRST
     */
    private async verifyStepOnDemand(stepAction: string, stepDetails?: string): Promise<VisionObservationResult> {
        try {
            const base64Image = await this.captureFrame();
            if (!base64Image) {
                return this.errorResult('–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞—Ö–æ–ø–∏—Ç–∏ –µ–∫—Ä–∞–Ω');
            }

            // STEP 1: Check if object is visible
            console.log('[GRISHA VISION] üëÅÔ∏è Checking object visibility first...');
            const visibilityCheck = await this.checkObjectVisibility(stepAction, base64Image);

            if (!visibilityCheck.visible) {
                console.warn(`[GRISHA VISION] ‚ö†Ô∏è Object not visible: ${visibilityCheck.message}`);
                const result: VisionObservationResult = {
                    type: 'alert',
                    message: visibilityCheck.message,
                    verified: false,
                    confidence: 0.9, // High confidence in "not seeing"
                    timestamp: Date.now(),
                    mode: 'on-demand'
                };
                this.emit('observation', result);
                return result;
            }

            console.log(`[GRISHA VISION] ‚úÖ Object visible: ${visibilityCheck.message}`);

            // STEP 2: Verify the action was completed
            const router = getProviderRouter();
            const response = await router.analyzeVision({
                image: base64Image,
                mimeType: 'image/jpeg',
                taskContext: stepAction,
                prompt: `–û–±'—î–∫—Ç –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ –≤–∏–¥–∏–º–∏–º: "${visibilityCheck.message}".

–¢–µ–ø–µ—Ä –ø–µ—Ä–µ–≤—ñ—Ä –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫—Ä–æ–∫—É: "${stepAction}". ${stepDetails || ''}

–ß–∏ –≤–∏–∫–æ–Ω–∞–Ω–æ —Ü—é –¥—ñ—é —É—Å–ø—ñ—à–Ω–æ? –©–æ —Å–∞–º–µ –∑–º—ñ–Ω–∏–ª–æ—Å—å –∞–±–æ –≤—ñ–¥–±—É–ª–æ—Å—å?`
            });

            this.frameCount++;

            const result: VisionObservationResult = {
                type: response.verified ? 'verification' : 'alert',
                message: response.analysis,
                verified: response.verified,
                confidence: response.confidence,
                anomalies: response.anomalies,
                timestamp: Date.now(),
                mode: 'on-demand'
            };

            this.emit('observation', result);
            console.log(`[GRISHA VISION] ${response.verified ? '‚úÖ' : '‚ö†Ô∏è'} Step verified (On-Demand): ${response.analysis.slice(0, 100)}`);

            return result;

        } catch (error: any) {
            console.error('[GRISHA VISION] Verification failed:', error);
            return this.errorResult(`–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: ${error.message}`);
        }
    }

    /**
     * Capture a single frame from selected source or screen
     */
    async captureFrame(overrideSourceId?: string): Promise<string | null> {
        try {
            const targetId = overrideSourceId || this.selectedSourceId;

            if (targetId) {
                // Capture specific window/screen
                const sources = await desktopCapturer.getSources({
                    types: ['window', 'screen'],
                    thumbnailSize: { width: 1280, height: 720 }
                });
                const source = sources.find(s => s.id === targetId);
                if (source) {
                    const jpegBuffer = source.thumbnail.toJPEG(85);
                    return jpegBuffer.toString('base64');
                }
            }

            // Fallback to full screen (first screen)
            const sources = await desktopCapturer.getSources({
                types: ['screen'],
                thumbnailSize: { width: 1280, height: 720 }
            });

            if (sources.length > 0) {
                const jpegBuffer = sources[0].thumbnail.toJPEG(85);
                return jpegBuffer.toString('base64');
            }

            return null;
        } catch (e) {
            console.error('[GRISHA VISION] Capture failed:', e);
            return null;
        }
    }

    // ==================== PRIVATE METHODS ====================

    /**
     * Start Live observation (Gemini Live streaming)
     */
    private async startLiveObservation(taskDescription?: string) {
        if (this.geminiLive && !this.geminiLive.isConnected) {
            try {
                await this.geminiLive.connect();
                await new Promise(resolve => setTimeout(resolve, 500));
            } catch (e) {
                console.error('[GRISHA VISION] Failed to connect Gemini Live:', e);
            }
        }

        if (this.geminiLive?.sendText) {
            this.geminiLive.sendText("–°–∏—Å—Ç–µ–º–∞: –ü–æ—á–∏–Ω–∞—î–º–æ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è. –û–ø–∏—à–∏ —â–æ –±–∞—á–∏—à –Ω–∞ –µ–∫—Ä–∞–Ω—ñ.");
        }

        await this.captureAndSendLiveFrame();

        this.captureInterval = setInterval(async () => {
            if (this.isSpeaking || this.isPaused) return;
            await this.captureAndSendLiveFrame();
        }, this.captureIntervalMs);

        this.emitResult('observation', `Live —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–æ–∑–ø–æ—á–∞—Ç–æ (${this.captureIntervalMs}ms). ${taskDescription || '–ú–æ–Ω—ñ—Ç–æ—Ä—é –≤–∏–∫–æ–Ω–∞–Ω–Ω—è...'}`);
    }

    /**
     * Capture and send frame to Gemini Live
     */
    private async captureAndSendLiveFrame() {
        try {
            const base64 = await this.captureFrame();
            if (base64 && this.geminiLive?.isConnected) {
                this.geminiLive.sendVideoFrame(base64);
                this.frameCount++;
            }
        } catch (e) {
            console.error('[GRISHA VISION] Live capture failed:', e);
        }
    }

    /**
     * Notify Gemini Live about an action (Live mode)
     */
    private async notifyActionLive(action: string, details: string) {
        if (this.geminiLive?.sendText) {
            console.log(`[GRISHA VISION] üó£Ô∏è Prompting verification: ${action}`);
            this.geminiLive.sendText(`–°–∏—Å—Ç–µ–º–∞: –í–∏–∫–æ–Ω–∞–Ω–æ –¥—ñ—é "${action}" (${details}). –ü—ñ–¥—Ç–≤–µ—Ä–¥—å —Å–ª–æ–≤–æ–º "–í–∏–∫–æ–Ω–∞–Ω–æ" –∞–±–æ –ø–æ–≤—ñ–¥–æ–º –ø—Ä–æ –ø–æ–º–∏–ª–∫—É.`);

            // Also send a fresh frame
            await this.captureAndSendLiveFrame();
        }
    }

    /**
     * Process response from Gemini Live
     */
    private processLiveResponse(text: string) {
        const lowerText = text.toLowerCase();
        let resultType: 'confirmation' | 'alert' | 'observation' = 'observation';

        if (lowerText.includes('alert') || lowerText.includes('–ø–æ–º–∏–ª–∫–∞') || lowerText.includes('error') || lowerText.includes('–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–æ')) {
            resultType = 'alert';
        } else if (lowerText.includes('ok') || lowerText.includes('–≤–∏–∫–æ–Ω–∞–Ω–æ') || lowerText.includes('stable') || lowerText.includes('done')) {
            resultType = 'confirmation';
        }

        const result: VisionObservationResult = {
            type: resultType,
            message: text,
            verified: resultType === 'confirmation',
            timestamp: Date.now(),
            mode: 'live'
        };

        console.log(`[GRISHA VISION] üîç ${resultType.toUpperCase()}: ${text}`);
        this.emit('observation', result);
    }

    /**
     * Helper: emit observation result
     */
    private emitResult(type: VisionObservationResult['type'], message: string, verified?: boolean) {
        this.emit('observation', {
            type,
            message,
            verified,
            timestamp: Date.now(),
            mode: this.mode
        } as VisionObservationResult);
    }

    /**
     * Helper: create error result
     */
    private errorResult(message: string): VisionObservationResult {
        return {
            type: 'alert',
            message,
            verified: false,
            timestamp: Date.now(),
            mode: this.mode
        };
    }

    get isActive(): boolean {
        return this.isObserving;
    }
}

// Singleton instance
let visionServiceInstance: GrishaVisionService | null = null;

export function getGrishaVisionService(): GrishaVisionService {
    if (!visionServiceInstance) {
        visionServiceInstance = new GrishaVisionService();
    }
    return visionServiceInstance;
}
