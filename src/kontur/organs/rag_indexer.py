#!/usr/bin/env python3
"""
RAG Indexer –¥–ª—è KONTUR v12 "–ö–æ–∑–∏—Ä"
–Ü–Ω–¥–µ–∫—Å—É—î AppleScript, JXA, —Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é macOS –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
"""
import os
import sys
from pathlib import Path
from typing import Generator, Tuple, List

# –û–ø—Ü—ñ–π–Ω–æ: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ MLX –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞ Apple Silicon
USE_MLX = os.getenv("USE_MLX", "0") in ("1", "true", "yes")
MLX_READY = False
try:
    if USE_MLX:
        import numpy as np
        from mlx_lm import load as mlx_load
        MLX_READY = True
except Exception:
    MLX_READY = False

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
KNOWLEDGE_SOURCES_DIRS = [
    os.path.expanduser("~/mac_assistant_rag/knowledge_sources"),
    os.path.expanduser("~/mac_assistant_rag/knowledge_base/large_corpus"),
]
CHROMA_PERSIST_DIR = os.path.expanduser("~/mac_assistant_rag/chroma_mac")
EMBEDDING_MODEL = "BAAI/bge-m3"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# === –í–ê–õ–Ü–î–ù–Ü –†–û–ó–®–ò–†–ï–ù–ù–Ø ===
VALID_EXTENSIONS = {
    '.applescript': 'AppleScript',
    '.scpt': 'AppleScript (compiled)',
    '.js': 'JXA',
    '.jxa': 'JXA',
    '.md': 'Documentation',
    '.txt': 'Text',
    '.sh': 'Shell Script'
}

def find_files() -> Generator[Tuple[Path, str, Path], None, None]:
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ñ–∞–π–ª–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –∑ —É—Å—ñ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π"""
    for source_dir in KNOWLEDGE_SOURCES_DIRS:
        source_path = Path(source_dir)
        if not source_path.exists():
            continue
        for ext, doc_type in VALID_EXTENSIONS.items():
            for path in source_path.rglob(f"*{ext}"):
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ node_modules, .git —Ç–∞ –≤–µ–ª–∏–∫—ñ —Ñ–∞–π–ª–∏
                if 'node_modules' in str(path) or '.git' in str(path):
                    continue
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ñ–∞–π–ª–∏ –±—ñ–ª—å—à—ñ 1MB (–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è 80GB —Å–∏—Ç—É–∞—Ü—ñ—è–º)
                try:
                    if path.stat().st_size > 1_000_000:
                        continue
                except:
                    continue
                yield path, doc_type, source_path

def read_file_safe(path: Path) -> str:
    """–ë–µ–∑–ø–µ—á–Ω–µ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∫–æ–¥—É–≤–∞–Ω–Ω—è–º–∏"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    for encoding in encodings:
        try:
            return path.read_text(encoding=encoding)
        except (UnicodeDecodeError, UnicodeError):
            continue
    return ""

def main():
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    
    console = Console()
    console.print("[bold green]üöÄ RAG Indexer v12 ‚Äî KONTUR '–ö–æ–∑–∏—Ä'[/bold green]")
    
    # === –ö–†–û–ö 1: –ó–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ ===
    console.print("\n[cyan]üìÇ –ü–æ—à—É–∫ —Ñ–∞–π–ª—ñ–≤...[/cyan]")
    files_to_index = list(find_files())
    console.print(f"[green]‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(files_to_index)} —Ñ–∞–π–ª—ñ–≤[/green]")
    
    if not files_to_index:
        console.print("[red]‚ùå –§–∞–π–ª–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ![/red]")
        return
    
    # === –ö–†–û–ö 2: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ ===
    console.print("\n[cyan]üìù –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...[/cyan]")
    
    from langchain_core.documents import Document
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    
    documents = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤...", total=len(files_to_index))
        
        for path, doc_type, base_path in files_to_index:
            content = read_file_safe(path)
            if not content or len(content.strip()) < 50:
                progress.advance(task)
                continue
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–æ–∫—É–º–µ–Ω—Ç –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
            try:
                source_name = str(path.relative_to(base_path))
            except ValueError:
                source_name = str(path.name)
            
            chunks = text_splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": source_name,
                        "type": doc_type,
                        "chunk": i,
                        "total_chunks": len(chunks)
                    }
                )
                documents.append(doc)
            
            progress.advance(task)
    
    console.print(f"[green]‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫—ñ–≤[/green]")
    
    # === –ö–†–û–ö 3: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Embeddings ===
    console.print("\n[cyan]üß† –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è embedding –º–æ–¥–µ–ª—ñ...[/cyan]")
    
    if MLX_READY:
        console.print("[green]‚ö° –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è MLX (bge-m3) –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞ Apple Silicon[/green]")
        model, tokenizer = mlx_load(EMBEDDING_MODEL)

        def embed_texts(texts: List[str]):
            outputs = []
            for t in texts:
                tokens = tokenizer(t, return_tensors="np", padding=True, truncation=True)
                hidden = model(**tokens).last_hidden_state  # (1, seq, dim)
                vec = hidden.mean(axis=1)[0]
                outputs.append(vec.tolist())
            return outputs

        embedding_fn = embed_texts
    else:
        console.print("[yellow]MLX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π –∞–±–æ USE_MLX –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é HuggingFaceEmbeddings.[/yellow]")
        from langchain_huggingface import HuggingFaceEmbeddings
        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

        def embedding_fn(texts: List[str]):
            return embedding_model.embed_documents(texts)
    
    # === –ö–†–û–ö 4: –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ Chroma ===
    console.print("\n[cyan]üíæ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ ChromaDB...[/cyan]")
    
    from langchain_chroma import Chroma
    
    # Batch indexing –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    BATCH_SIZE = 100
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è...", total=len(documents))
        
        db = Chroma(
            persist_directory=CHROMA_PERSIST_DIR,
            embedding_function=embedding_fn
        )
        
        for i in range(0, len(documents), BATCH_SIZE):
            batch = documents[i:i + BATCH_SIZE]
            db.add_documents(batch)
            progress.advance(task, advance=len(batch))
    
    # === –§–Ü–ù–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢ ===
    console.print("\n" + "="*50)
    console.print("[bold green]‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê![/bold green]")
    console.print(f"[cyan]üìä –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–æ–¥–∞–Ω–æ: {len(documents)}[/cyan]")
    console.print(f"[cyan]üìÅ –ë–∞–∑–∞: {CHROMA_PERSIST_DIR}[/cyan]")
    console.print("="*50)

if __name__ == "__main__":
    main()
