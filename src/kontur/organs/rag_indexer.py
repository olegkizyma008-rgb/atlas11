#!/usr/bin/env python3
"""
RAG Indexer –¥–ª—è KONTUR v12 "–ö–æ–∑–∏—Ä" ‚Äî MLX Edition
–Ü–Ω–¥–µ–∫—Å—É—î AppleScript, JXA, —Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é macOS –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
Semantic chunking + –∫–æ–Ω—Ç–µ–∫—Å—Ç + hierarchical indexing + GPU acceleration
"""
import os
import sys
import uuid
import json
from pathlib import Path
from typing import Generator, Tuple, List, Dict, Any

# –û–ø—Ü—ñ–π–Ω–æ: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ MLX –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞ Apple Silicon
USE_MLX = os.getenv("USE_MLX", "1") in ("1", "true", "yes")
MLX_READY = False
try:
    if USE_MLX:
        import numpy as np
        from mlx_lm import load as mlx_load
        MLX_READY = True
except Exception:
    MLX_READY = False

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
KNOWLEDGE_SOURCES_DIRS = [
    "/Users/dev/Documents/GitHub/atlas/rag/knowledge_sources",
    "/Users/dev/Documents/GitHub/atlas/rag/knowledge_base/large_corpus",
]
CHROMA_PERSIST_DIR = "/Users/dev/Documents/GitHub/atlas/rag/chroma_mac"
EMBEDDING_MODEL = "BAAI/bge-m3"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
SEMANTIC_SIMILARITY_THRESHOLD = 0.5  # Threshold –¥–ª—è semantic chunking

# === KONTUR URN ===
URN = "kontur://organ/rag-indexer"

# === –í–ê–õ–Ü–î–ù–Ü –†–û–ó–®–ò–†–ï–ù–ù–Ø ===
VALID_EXTENSIONS = {
    '.applescript': 'AppleScript',
    '.scpt': 'AppleScript (compiled)',
    '.js': 'JXA',
    '.jxa': 'JXA',
    '.md': 'Documentation',
    '.txt': 'Text',
    '.sh': 'Shell Script'
}

def find_files() -> Generator[Tuple[Path, str, Path], None, None]:
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ñ–∞–π–ª–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –∑ —É—Å—ñ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π"""
    for source_dir in KNOWLEDGE_SOURCES_DIRS:
        source_path = Path(source_dir)
        if not source_path.exists():
            continue
        for ext, doc_type in VALID_EXTENSIONS.items():
            for path in source_path.rglob(f"*{ext}"):
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ node_modules, .git —Ç–∞ –≤–µ–ª–∏–∫—ñ —Ñ–∞–π–ª–∏
                if 'node_modules' in str(path) or '.git' in str(path):
                    continue
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ñ–∞–π–ª–∏ –±—ñ–ª—å—à—ñ 1MB (–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è 80GB —Å–∏—Ç—É–∞—Ü—ñ—è–º)
                try:
                    if path.stat().st_size > 1_000_000:
                        continue
                except:
                    continue
                yield path, doc_type, source_path

def read_file_safe(path: Path) -> str:
    """–ë–µ–∑–ø–µ—á–Ω–µ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∫–æ–¥—É–≤–∞–Ω–Ω—è–º–∏"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    for encoding in encodings:
        try:
            return path.read_text(encoding=encoding)
        except (UnicodeDecodeError, UnicodeError):
            continue
    return ""

def extract_file_context(content: str, max_length: int = 300) -> str:
    """–í–∏—Ç—è–≥—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ–∞–π–ª—É (–ø–µ—Ä—à—ñ —Ä—è–¥–∫–∏)"""
    lines = content.split('\n')[:5]
    context = '\n'.join(lines)
    return context[:max_length]

def create_semantic_chunks(content: str, embeddings_fn) -> List[str]:
    """–†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ semantic chunks –Ω–∞ –æ—Å–Ω–æ–≤—ñ embedding similarity"""
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    
    base_chunks = text_splitter.split_text(content)
    
    if len(base_chunks) <= 1:
        return base_chunks
    
    try:
        chunk_embeddings = embeddings_fn(base_chunks)
        
        import numpy as np
        semantic_chunks = []
        current_chunk = base_chunks[0]
        
        for i in range(1, len(base_chunks)):
            emb1 = np.array(chunk_embeddings[i-1])
            emb2 = np.array(chunk_embeddings[i])
            
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            
            if similarity < SEMANTIC_SIMILARITY_THRESHOLD:
                semantic_chunks.append(current_chunk)
                current_chunk = base_chunks[i]
            else:
                current_chunk += "\n\n" + base_chunks[i]
        
        semantic_chunks.append(current_chunk)
        return semantic_chunks
    except Exception:
        return base_chunks

def main():
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    
    console = Console()
    console.print("[bold green]üöÄ RAG Indexer v12 ‚Äî KONTUR '–ö–æ–∑–∏—Ä'[/bold green]")
    
    # === –ö–†–û–ö 1: –ó–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ ===
    console.print("\n[cyan]üìÇ –ü–æ—à—É–∫ —Ñ–∞–π–ª—ñ–≤...[/cyan]")
    files_to_index = list(find_files())
    console.print(f"[green]‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(files_to_index)} —Ñ–∞–π–ª—ñ–≤[/green]")
    
    if not files_to_index:
        console.print("[red]‚ùå –§–∞–π–ª–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ![/red]")
        return
    
    # === –ö–†–û–ö 2: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Embeddings (–ø–æ—Ç—Ä–µ–±—É—î–º–æ –¥–ª—è semantic chunking) ===
    console.print("\n[cyan]üß† –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è embedding –º–æ–¥–µ–ª—ñ...[/cyan]")
    
    embeddings_fn = None
    # –õ–æ–∫–∞–ª—å–Ω–∏–π –ø—Ä–∞–ø–æ—Ä–µ—Ü—å, —â–æ–± –Ω–µ –ª–∞–º–∞—Ç–∏ –≥–ª–æ–±–∞–ª –ø—Ä–∏ fallback
    mlx_ready = MLX_READY

    if mlx_ready:
        console.print("[green]‚ö° –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è MLX (bge-m3) –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞ Apple Silicon[/green]")
        try:
            model, tokenizer = mlx_load(EMBEDDING_MODEL)

            def embed_texts(texts: List[str]):
                outputs = []
                for t in texts:
                    tokens = tokenizer(t, return_tensors="np", padding=True, truncation=True)
                    hidden = model(**tokens).last_hidden_state
                    vec = hidden.mean(axis=1)[0]
                    outputs.append(vec.tolist())
                return outputs

            embeddings_fn = embed_texts
        except FileNotFoundError as e:
            console.print(f"[yellow]‚ö†Ô∏è MLX –Ω–µ –∑–º—ñ–≥ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å ({e}). –ü–µ—Ä–µ—Ö–æ–¥–∂—É –Ω–∞ HuggingFaceEmbeddings.[/yellow]")
            mlx_ready = False

    if embeddings_fn is None:
        console.print("[yellow]MLX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é HuggingFaceEmbeddings.[/yellow]")
        from langchain_huggingface import HuggingFaceEmbeddings
        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        embeddings_fn = embedding_model.embed_documents
    
    # === –ö–†–û–ö 3: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ semantic chunking + –∫–æ–Ω—Ç–µ–∫—Å—Ç + hierarchical indexing ===
    console.print("\n[cyan]üìù –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (semantic + –∫–æ–Ω—Ç–µ–∫—Å—Ç + —ñ—î—Ä–∞—Ä—Ö—ñ—è)...[/cyan]")
    
    from langchain_core.documents import Document
    
    documents = []
    document_hierarchy = {}  # –î–ª—è hierarchical indexing
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤...", total=len(files_to_index))
        
        for path, doc_type, base_path in files_to_index:
            content = read_file_safe(path)
            if not content or len(content.strip()) < 50:
                progress.advance(task)
                continue
            
            try:
                source_name = str(path.relative_to(base_path))
            except ValueError:
                source_name = str(path.name)
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ ID –¥–ª—è —ñ—î—Ä–∞—Ä—Ö—ñ—ó
            document_id = str(uuid.uuid4())
            file_context = extract_file_context(content)
            
            # Semantic chunking
            chunks = create_semantic_chunks(content, embeddings_fn)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—î—Ä–∞—Ä—Ö—ñ—é
            document_hierarchy[document_id] = {
                "source": source_name,
                "type": doc_type,
                "total_chunks": len(chunks),
                "file_context": file_context
            }
            
            for i, chunk in enumerate(chunks):
                # –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ç–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π chunk
                prev_chunk = chunks[i-1][-150:] if i > 0 else ""
                next_chunk = chunks[i+1][:150] if i < len(chunks)-1 else ""
                
                # Hierarchical metadata
                chunk_id = str(uuid.uuid4())
                hierarchy_path = f"{source_name}/chunk_{i}"
                
                doc = Document(
                    page_content=chunk,
                    metadata={
                        # –ë–∞–∑–æ–≤—ñ –º–µ—Ç–∞–¥–∞–Ω–Ω—ñ
                        "source": source_name,
                        "type": doc_type,
                        "chunk": i,
                        "total_chunks": len(chunks),
                        
                        # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                        "file_context": file_context,
                        "prev_chunk_context": prev_chunk,
                        "next_chunk_context": next_chunk,
                        
                        # Hierarchical indexing
                        "document_id": document_id,
                        "chunk_id": chunk_id,
                        "hierarchy_path": hierarchy_path,
                        "hierarchy_level": "chunk",
                        "hierarchy_depth": 2,  # document -> chunk
                        
                        # KONTUR –º–µ—Ç–∞–¥–∞–Ω–Ω—ñ
                        "kontur_urn": URN,
                        "indexed_at": str(Path(path).stat().st_mtime),
                    }
                )
                documents.append(doc)
            
            progress.advance(task)
    
    console.print(f"[green]‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫—ñ–≤ (semantic + –∫–æ–Ω—Ç–µ–∫—Å—Ç + —ñ—î—Ä–∞—Ä—Ö—ñ—è)[/green]")
    
    # === –ö–†–û–ö 4: –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ Chroma –∑ MLX ===
    console.print("\n[cyan]üíæ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ ChromaDB (–∑ MLX GPU acceleration)...[/cyan]")
    
    from langchain_chroma import Chroma
    
    # Batch indexing –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    BATCH_SIZE = 100
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è...", total=len(documents))
        
        db = Chroma(
            persist_directory=CHROMA_PERSIST_DIR,
            embedding_function=embeddings_fn if mlx_ready else None
        )
        
        for i in range(0, len(documents), BATCH_SIZE):
            batch = documents[i:i + BATCH_SIZE]
            db.add_documents(batch)
            progress.advance(task, advance=len(batch))
    
    # === –ö–†–û–ö 5: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—î—Ä–∞—Ä—Ö—ñ—ó ===
    console.print("\n[cyan]üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—î—Ä–∞—Ä—Ö—ñ—ó –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...[/cyan]")
    hierarchy_file = Path(CHROMA_PERSIST_DIR) / "hierarchy.json"
    hierarchy_file.parent.mkdir(parents=True, exist_ok=True)
    with open(hierarchy_file, 'w', encoding='utf-8') as f:
        json.dump(document_hierarchy, f, ensure_ascii=False, indent=2)
    
    # === –§–Ü–ù–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢ ===
    console.print("\n" + "="*60)
    console.print("[bold green]‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê![/bold green]")
    console.print(f"[cyan]üìä –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–æ–¥–∞–Ω–æ: {len(documents)}[/cyan]")
    console.print(f"[cyan]üìÅ –ë–∞–∑–∞: {CHROMA_PERSIST_DIR}[/cyan]")
    console.print(f"[cyan]üß† Embedding –º–æ–¥–µ–ª—å: {EMBEDDING_MODEL}[/cyan]")
    console.print(f"[cyan]‚ö° GPU acceleration: {'MLX (M1 Max)' if mlx_ready else 'CPU'}[/cyan]")
    console.print(f"[cyan]üîÄ Semantic chunking: ‚úÖ –£–í–Ü–ú–ö–ù–ï–ù–û[/cyan]")
    console.print(f"[cyan]üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç: ‚úÖ –î–û–î–ê–ù–û (prev/next/file)[/cyan]")
    console.print(f"[cyan]üìä Hierarchical indexing: ‚úÖ –î–û–î–ê–ù–û ({len(document_hierarchy)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)[/cyan]")
    console.print(f"[cyan]üèõÔ∏è KONTUR —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è: ‚úÖ URN={URN}[/cyan]")
    console.print("="*60)

if __name__ == "__main__":
    main()
